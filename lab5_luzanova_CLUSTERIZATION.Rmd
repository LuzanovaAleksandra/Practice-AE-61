---
title: "lab5_luzanova_CLUSTERIZATION"
output:
  word_document: default
  html_notebook: default
  html_document: default
---

# Download the data
```{r}
library (psych)
set.seed(123)
f <- read.csv2('crime_index_pre.csv', header = TRUE, encoding = 'UNICOD')
f= subset(f, select = -c(1))
head(f)
describe(f)
```
#Висновок: для побудови карт Кохонена використані дані, а саме: індекс злочинності та економічні показники. кількість спостережень – 123, кількість змінних – 13

# Hierarchical clustering
```{r}
model_hc <- hclust(dist(f), method = "ward.D" )
plot(model_hc, main = paste('Dendrogram'))
```
#Висновок: на основі навчальної вибірки побудовано дендрограму з використанням методу Ward.D.

## Fitting HC to the dataset
```{r}
y_hc <- cutree(model_hc, k = 3)
#cluster cores
aggregate(f,by=list(y_hc),FUN=mean)
#Cluster stat
f$hc <- y_hc
table(f$hc)
```
#Висновок: на основі аналізу дендрограми виявлено три кластери. Розраховано характеристики типового об’єкту кластерів. До першого кластеру потрапили 85 спостережень, до другого - 24, до третього - 14. До першого, найбільшого кластеру війшли країни з рівнем злочинності вище середнього, малою кількістю населення, малим ВВП на душу населення, з середнім рівнем безробіття, з низкою тривалістю життя, з низьким рівнем смертності, з високим рівнем народжуванності, з рівнем зайнятості вище середнього, з нидьким рівнем зайнятості у сфері послуг та промисловості, з високим рівнем зайнятості у с/г, з низьким рівнем видатків у сферу освіти та збройних сил

## Plotting the dendrogram
```{r}
plot(model_hc, cex = 0.7, labels = FALSE)
rect.hclust(model_hc, k = 3, border = 2:5)
```

## Visualising the clusters
```{r}
library(cluster)
clusplot(f[,c('edu_ex','Population')],
         y_hc,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels= 0,
         plotchar = FALSE,
         span = TRUE,
         main = paste('Clusters of customers'),
         xlab = 'edu_ex',
         ylab = 'Population')
```
#бачимо, що всі три кластери добре поділені
```{r}
y_hc <- cutree(model_hc, k = 3)
#cluster cores
aggregate(f,by=list(y_hc),FUN=mean)
f$hc <- y_hc
table(f$hc)

```
```{r}
f= subset(f, select = -c(hc)) #видалимо стовпець hc
```
#"K-Means"

## Historgram for each attribute
```{r}
library(tidyr)
library(ggplot2)
f %>% 
  gather(Attributes, value, 1:13) %>% 
  ggplot(aes(x=value)) +
  geom_histogram(fill = "lightblue2", color = "black") + 
  facet_wrap(~Attributes, scales = "free_x") +
  labs(x = "Value", y = "Frequency")
```

## Correlation
```{r}
library(corrplot)
corrplot(cor(f), type = "upper", method = "ellipse", tl.cex = 0.9)
```
#Бачимо, що змінні між собою корелюють у допустимих межах (низький та нижче середнього рівня), але й такі, що мають високу кореляцію, а саме:life_ex/bir, life_ex/emp_a, emp_s/emp_a, edu_ex/mil_ex 
## Clustree
```{r}
library(clustree)
library(dplyr)
tmp <- NULL
for (k in 1:8){
  tmp[k] <- kmeans(f, k, nstart = 30)
}
df <- data.frame(tmp)
# add a prefix to the column names
colnames(df) <- seq(1:8
                    )
colnames(df) <- paste0("k",colnames(df))
# get individual PCA
df.pca <- prcomp(df, center = TRUE, scale. = FALSE)
ind.coord <- df.pca$x
ind.coord <- ind.coord[,1:2]
df <- bind_cols(as.data.frame(df), as.data.frame(ind.coord))
clustree(df, prefix = "k")
```
##Бачимо, що при розбитті на 2 кластери - маємо один кластер з більшою кількістю спостережень, інший - з меншою. важливо виділити те, що більшість спостережень, при кластеризації, потрапляють до одного кластеру. Більш рівномірна кількість спостережень у кластері буде 6 кластерах

# Fitting K-Means to the dataset
```{r}
set.seed(29)
model_km = kmeans(f, 3)
#cluster cores
y_km = model_km$cluster
aggregate(f,by=list(y_km),FUN=mean)
```

# Visualising the clusters
```{r}
library(cluster)
clusplot(f[,c('edu_ex','Population')],
         y_km,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels= 0,
         plotchar = FALSE,
         span = TRUE,
         main = paste('Clusters of customers'),
         xlab = 'edu_ex',
         ylab = 'Population')
```

# Comparing to HC
```{r}
library(clusteval)
cluster_similarity(y_hc,y_km)
```
#Аналіз моделей показує, що спостереження стійкі до кластеризації, бо розбиття на кластери за першим та другим методами є подібними на 92.5%
```{r}
mPopulation <- mean(f$Population)
sPopulation <- sd(f$Population)
f$Population <- (f$Population-mPopulation)/sPopulation

mgdp_p <- mean(f$gdp_p)
sgdp_p <- sd(f$gdp_p)
f$gdp_p<- (f$gdp_p-mgdp_p)/sgdp_p

munem <- mean(f$unem)
sunem <- sd(f$unem)
f$unem <- (f$unem-munem)/sunem

mlife_ex <- mean(f$life_ex)
slife_ex <- sd(f$life_ex)
f$life_ex <- (f$life_ex-mlife_ex)/slife_ex

mdea <- mean(f$dea)
sdea <- sd(f$dea)
f$dea<- (f$dea-mdea)/sdea

mbir <- mean(f$bir)
sbir <- sd(f$bir)
f$bir <- (f$bir-mbir)/sbir

memp <- mean(f$emp)
semp <- sd(f$emp)
f$emp <- (f$emp-memp)/semp

memp_s <- mean(f$emp_s)
semp_s <- sd(f$emp_s)
f$emp_s<- (f$emp_s-memp_s)/semp_s

memp_a <- mean(f$emp_a)
semp_a <- sd(f$emp_a)
f$emp_a <- (f$emp_a-memp_a)/semp_a

memp_i <- mean(f$emp_i)
semp_i <- sd(f$emp_i)
f$emp_i <- (f$emp_i-memp_i)/semp_i

medu_ex <- mean(f$edu_ex)
sedu_ex <- sd(f$edu_ex)
f$edu_ex<- (f$edu_ex-medu_ex)/sedu_ex

mmil_ex <- mean(f$mil_ex)
smil_ex <- sd(f$mil_ex)
f$mil_ex <- (f$mil_ex-mmil_ex)/smil_ex

f_matrix <- as.matrix(f)
head (f)
```
#Висновок: шкалювання кількісних змінних

# NbCLust
```{r}

library(factoextra)
library(NbClust)
res.nbclust <- NbClust(f, distance = "euclidean",
                  min.nc = 2, max.nc = 10, 
                  method = "complete", index ="all")
fviz_nbclust(res.nbclust) + theme_minimal() + ggtitle("NbClust's optimal number of clusters")
# Elbow method
# The sum of squares at each number of clusters is calculated and graphed, and the user looks for a change of slope from steep to shallow (an elbow) to determine the optimal number of clusters.
fviz_nbclust(f, kmeans, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle = "Elbow method")
# Silhouette method
# The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k.
fviz_nbclust(f, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
# Gap statistic
# The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic. This means that the clustering structure is far away from the random uniform distribution of points.
fviz_nbclust(f, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")
```
##бачимо з аналізу, що 9 голосів надано за 3 кластери, 8 голосів за 4 кластери. Зупинимось на 3 кластерах.

#"Kohonen maps"
# Features Scaling

#Висновок: моделі класифікації вимагають попереднього шкалювання кількісних змінних. Шкалювання виконано.

# Fitting the NN
```{r}
set.seed(123)
library(kohonen)
som_grid <- somgrid(xdim = 5, ydim = 3, topo = "hexagonal") 
som_model <- som(f_matrix, grid = som_grid, rlen = 200,
                 alpha = c(0.05,0.01), keep.data = TRUE)
plot(som_model, type = "changes")
```
#Висновок: модель Кохонена потребуює шкалювання, шкалювання кількісних змінних. Шкалювання виконано.
#Висновок: графік зміни помилки нейронної мережі Кохонена свідчить про успішне навчання моделі.

## Visualization
```{r}
#Palette
coolBlueHotRed <- function(n, alpha = 1) {
    rainbow(n, end = 4/6, alpha = alpha)[n:1] 
}
par(mfrow = c(1, 2))
#Number of objects at sells
plot(som_model, type = "counts", palette.name = coolBlueHotRed)
#Distance to core
plot(som_model, type = "quality", palette.name = coolBlueHotRed)
```
#Висновок: в кожну клітинку мережі потрапило від 2 до 16 спостережень, мережа досить повна.

## Maps of the factors
```{r}
plot(som_model, type = "codes")
```
#Висновок: ця візуалізація дозволяє аналізувати всі фактори на одній карті.

```{r}
par(mfrow = c(4, 3))

plot(som_model, type = "property", 
     property = som_model$codes[[1]][,1],
     main = "Population",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,2], 
     main = "gdp_p",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,3], 
     main = "unem",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,4], 
     main = "life_ex",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,5], 
     main = "dea",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,6], 
     main = "bir",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,7], 
     main = "emp",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,8], 
     main = "emp_s",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,9], 
     main = "emp_a",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,10], 
     main = "emp_i",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,11], 
     main = "edu_ex",
     palette.name = coolBlueHotRed)
plot(som_model, type = "property", 
     property = som_model$codes[[1]][,12], 
     main = "mil_ex",
     palette.name = coolBlueHotRed)
```
#Висновок: ця візуалізація дозволяє аналізувати всі фактори на окремих картах. 

# Clusters description
```{r}
mydata <- as.matrix(som_model$codes[[1]])
#Use hierarchical clustering, k=3
som_cluster <- cutree(hclust(dist(mydata)), 3)
#Palette
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c',
                    '#d62728', '#9467bd', '#8c564b', '#e377c2')
#Colored clusters
plot(som_model, type = "codes", 
     bgcol = pretty_palette[som_cluster])
add.cluster.boundaries(som_model, som_cluster) 
```
#Висновок: ця візуалізація дозволяє аналізувати парамерти двох виявлених кластерів на одній карті. 
#```{r}

f$Population <- f$Population*sPopulation
f$Population <- f$Population+mPopulation

f$gdp_p<- f$gdp_p*sgdp_p
f$gdp_p<- f$gdp_p+mgdp_p

f$unem <- f$unem*sunem
f$unem <- f$unem+munem

f$life_ex <- f$life_ex*slife_ex
f$life_ex <- f$life_ex+mlife_ex

f$dea<- f$dea*sdea
f$dea<- f$dea+mdea

f$bir <- f$bir*sbir
f$bir <- f$bir+mbir

f$emp <- f$emp*semp
f$emp <- f$emp+memp

f$emp_s<- f$emp_s*semp_s
f$emp_s<- f$emp_s+memp_s

f$emp_a <- f$emp_a*semp_a
f$emp_a <- f$emp_a+memp_a

f$emp_i <- f$emp_i*semp_i
f$emp_i <- f$emp_i+memp_i

f$edu_ex<- f$edu_ex*sedu_ex
f$edu_ex<- f$edu_ex+medu_ex

f$mil_ex <- f$mil_ex*smil_ex
f$mil_ex <- f$mil_ex+mmil_ex

head (f)
#```

```{r}
aggregate(mydata,by=list(som_cluster),FUN=mean)
```
#Висновок: на основі нейроних мереж Кохонена виявлено три кластери. Розраховано характеристики типових об’єктів кластерів.
#Отже, кластерізацію проведено за трьома методами , а саме: Ієрархічна кластеризація, Кластеризація на основі k-means та на основі нейроних мереж Кохонена. Аналіз моделей показує, що спостереження стійкі до кластеризації, бо розбиття на кластери за першим та другим методами є подібними на 92.4%. За 3-ма методами було виділено 3 кластери.

